{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "生成词向量(word embedding)的另一种方法是Word2Vec\n",
    "\n",
    "Word2Vec包含了两种词训练模型：CBOW模型和Skip-gram模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW MODEL\n",
    "\n",
    "但在实际使用时输入不一定是one-hot编码，也可以是随机生成的N维向量\n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"oneHot.jpg\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"avgVec.jpg\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"fc.jpg\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"softMax.jpg\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Feature vector of a review</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取数据\n",
    "第一列是评分1～5\\\n",
    "第二列是评论标题\\\n",
    "第三列是评论正文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mens ultrasheer</td>\n",
       "      <td>This model may be ok for sedentary types, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Surprisingly delightful</td>\n",
       "      <td>This is a fast read filled with unexpected hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Works, but not as advertised</td>\n",
       "      <td>I bought one of these chargers..the instructio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Oh dear</td>\n",
       "      <td>I was excited to find a book ostensibly about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Incorrect disc!</td>\n",
       "      <td>I am a big JVC fan, but I do not like this mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                             1  \\\n",
       "0  1               mens ultrasheer   \n",
       "1  4       Surprisingly delightful   \n",
       "2  2  Works, but not as advertised   \n",
       "3  2                       Oh dear   \n",
       "4  2               Incorrect disc!   \n",
       "\n",
       "                                                   2  \n",
       "0  This model may be ok for sedentary types, but ...  \n",
       "1  This is a fast read filled with unexpected hum...  \n",
       "2  I bought one of these chargers..the instructio...  \n",
       "3  I was excited to find a book ostensibly about ...  \n",
       "4  I am a big JVC fan, but I do not like this mod...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "test = pd.read_csv('test.csv',header=None)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Inspiring</td>\n",
       "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chrono Cross OST</td>\n",
       "      <td>The music of Yasunori Misuda is without questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Too good to be true</td>\n",
       "      <td>Probably the greatest soundtrack in history! U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                      1  \\\n",
       "0  3                     more like funchuck   \n",
       "1  5                              Inspiring   \n",
       "2  5  The best soundtrack ever to anything.   \n",
       "3  4                       Chrono Cross OST   \n",
       "4  5                    Too good to be true   \n",
       "\n",
       "                                                   2  \n",
       "0  Gave this to my dad for a gag gift after direc...  \n",
       "1  I hope a lot of people hear this cd. We need m...  \n",
       "2  I'm reading a lot of reviews saying that this ...  \n",
       "3  The music of Yasunori Misuda is without questi...  \n",
       "4  Probably the greatest soundtrack in history! U...  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv',header=None)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test data is  650000\n",
      "the score of: \" This is a fast read filled with unexpected humour and profound insights into the art of politics and policy. In brief, it is sly, wry, and wise. \"is  4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_data=test[2]\n",
    "test_label=test[0]\n",
    "print('size of test data is ',len(test_data))\n",
    "print('the score of: \"',test_data[1],'\"is ',test_label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data is  3000000\n",
      "the score of: \" Gave this to my dad for a gag gift after directing \"Nunsense,\" he got a reall kick out of it! \"is  3\n"
     ]
    }
   ],
   "source": [
    "train_data=train[2]\n",
    "train_label=train[0]\n",
    "print('size of train data is ',len(train_data))\n",
    "print('the score of: \"',train_data.iloc[0],'\"is ',train_label.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据\n",
    "只保留单词；将字母处理为小写；删掉停用词；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing:  Gave this to my dad for a gag gift after directing \"Nunsense,\" he got a reall kick out of it!\n",
      "After processing:  gave dad gag gift directing nunsense got reall kick\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "def cleanText(raw_text,remove_stopwords=True,sentences=False):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    text=letters_only.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    if sentences:\n",
    "        return \" \".join(text)\n",
    "    else:\n",
    "        return text\n",
    "text=cleanText(train_data.iloc[0],sentences=True)\n",
    "print(\"Before processing: \",train_data.iloc[0])\n",
    "print(\"After processing: \",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重采样取部分数据实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "t_data,t_label=[],[]\n",
    "for i in range(30000):\n",
    "    idx=np.random.randint(len(train_data))\n",
    "    t_data.append(train_data[idx])\n",
    "    t_label.append(train_label[idx])\n",
    "\n",
    "test,label=[],[]\n",
    "for i in range(1000):\n",
    "    idx=np.random.randint(len(test_data))\n",
    "    test.append(test_data[idx])\n",
    "    label.append(test_label[idx])\n",
    "\n",
    "len(t_label)\n",
    "# t_data,t_label=train_data[:],train_label[:]\n",
    "# test,label=test_data[:],test_label[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5408.64it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 5819.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_clean,test_clean=[],[]\n",
    "for i in tqdm(range(len(t_data))):\n",
    "    train_clean.append(cleanText(t_data[i],remove_stopwords=True,sentences=True))\n",
    "for i in tqdm(range(len(test))):\n",
    "    test_clean.append(cleanText(test[i],remove_stopwords=True,sentences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/user/miniconda3/lib/python3.8/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用gensim库构建Word2Vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将训练集分词后整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 parsed sentence in the training set\n",
      "\n",
      "Original review : \n",
      " This comes in two peices and you connect them. It is supper cheap and plastic. But what can you expect for the price. On the other side it works and for the price I would by more and recommend them. However, i would not recommend putting belts on it, Just ties. Also you have to make sure you keep it balanced or it will hang very crooked. So when you hang ties on it make sure you spread them out to balance it out. Other then that i recommend it. 3.5 / 5\n",
      "\n",
      "Cleaned review : \n",
      " comes two peices connect supper cheap plastic expect price side works price would recommend however would recommend putting belts ties also make sure keep balanced hang crooked hang ties make sure spread balance recommend\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['comes', 'two', 'peices', 'connect', 'supper', 'cheap', 'plastic', 'expect', 'price', 'side', 'works', 'price', 'would', 'recommend', 'however', 'would', 'recommend', 'putting', 'belts', 'ties', 'also', 'make', 'sure', 'keep', 'balanced', 'hang', 'crooked', 'hang', 'ties', 'make', 'sure', 'spread', 'balance', 'recommend']\n"
     ]
    }
   ],
   "source": [
    "def parseSent(review,sentences,remove_stopwords=True):\n",
    "    temp=cleanText(review, remove_stopwords=True,sentences=False)\n",
    "    sentences.append(temp)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in train_clean:\n",
    "    parseSent(review, sentences)\n",
    "\n",
    "\n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
    "\n",
    "print('Original review : \\n',t_data[19])\n",
    "print()\n",
    "print('Cleaned review : \\n', train_clean[19])\n",
    "print()\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[19])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练Word2Vec模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数调节\n",
    "sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法，使用skip-gram，训练速度慢，但对罕见字有效\n",
    "\n",
    "size：是指特征向量的维度，默认为100\n",
    "\n",
    "window：窗口大小，表示当前词与预测词在一个句子中的最大距离是多少，一般CBOW选择5，SG选择10\n",
    "\n",
    "min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "\n",
    "sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)\n",
    "\n",
    "workers：用于控制训练的并行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit parsed sentences to Word2Vec model \n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "num_features = 300  #embedding dimension  \n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=10, size=num_features, min_count = 5,\\\n",
    "                 window = 5, sample = 1e-3 , hs=1,sg=1)\n",
    "w2v.init_sims(replace=True)\n",
    "# w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary list : 15527 \n",
      "\n",
      "Show first 10 words in the vocalbulary list  vocabulary list: \n",
      " ['book', 'one', 'like', 'good', 'would', 'great', 'get', 'read', 'time', 'really']\n",
      "\n",
      "Part of vector of the word \"book\": \n",
      " [-0.04047463  0.08282275 -0.04423171  0.0444067   0.03222555]\n",
      "\n",
      "The size of words vector : \n",
      " (300,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-152-83a545dc014d>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  print('Part of vector of the word \"book\": \\n',w2v['book'][:5])\n",
      "<ipython-input-152-83a545dc014d>:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  print('The size of words'' vector : \\n',w2v['book'].shape)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) \n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])\n",
    "print()\n",
    "print('Part of vector of the word \"book\": \\n',w2v['book'][:5])\n",
    "print()\n",
    "print('The size of words'' vector : \\n',w2v['book'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求出该review每个可用词的向量求平均值作为该review的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom the training data into feature vectors\n",
    "def makeFeatureVec(review, model, num_features):\n",
    "    featureVec = np.zeros(num_features)\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word) \n",
    "    isZeroVec = True\n",
    "    for word in review:\n",
    "        if word in index2word_set: \n",
    "            nwords += 1\n",
    "            # 每个word更新该review的向量\n",
    "            featureVec += model[word]\n",
    "            isZeroVec = False\n",
    "    if isZeroVec == False:\n",
    "        featureVec /= nwords\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整合数据集所有review向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    '''\n",
    "    Transform all reviews to feature vectors using makeFeatureVec()\n",
    "    '''\n",
    "    reviewFeatureVecs = []\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs.append(makeFeatureVec(review, model,num_features))\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将训练集测试集review转成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-153-d0a2fa488d2e>:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec += model[word]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 6272.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : 30000  feature vectors with 300 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set : 1000  feature vectors with 300 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get feature vectors for training set\n",
    "X_train_cleaned = sentences\n",
    "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\n",
    "print('Training set :',len(trainVector),' feature vectors with',len(trainVector[0]) ,'dimensions')\n",
    "\n",
    "\n",
    "# Get feature vectors for validation set\n",
    "X_test_cleaned = []\n",
    "for review in tqdm(test_clean):\n",
    "    X_test_cleaned.append(cleanText(review, remove_stopwords=True,sentences=False))\n",
    "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\n",
    "print('Testing set :',len(testVector),' feature vectors with',len(testVector[0]) ,'dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review : \n",
      " This comes in two peices and you connect them. It is supper cheap and plastic. But what can you expect for the price. On the other side it works and for the price I would by more and recommend them. However, i would not recommend putting belts on it, Just ties. Also you have to make sure you keep it balanced or it will hang very crooked. So when you hang ties on it make sure you spread them out to balance it out. Other then that i recommend it. 3.5 / 5\n",
      "\n",
      "Part of vector : \n",
      " [-0.02030254  0.02650279  0.01736031  0.03576423  0.02310178  0.01488558\n",
      " -0.02948713  0.02083334  0.01179845 -0.03702418]\n",
      "\n",
      "Size of vector : \n",
      " (300,)\n"
     ]
    }
   ],
   "source": [
    "print('Original review : \\n',t_data[19])\n",
    "print()\n",
    "print('Part of vector : \\n',trainVector[19][:10])\n",
    "print()\n",
    "print('Size of vector : \\n',trainVector[19].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到词向量后用随机森林训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 3])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100,max_depth=17)\n",
    "rf.fit(trainVector, t_label)\n",
    "predictions = rf.predict(testVector)\n",
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2Vaccuracy(pred,label):\n",
    "    right=0\n",
    "    for p,a in zip(pred,label):\n",
    "        if p>=4 and a>=4:\n",
    "            right+=1\n",
    "        elif p<4 and a<4:\n",
    "            right+=1\n",
    "    return 100*right/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=  74.6 %\n"
     ]
    }
   ],
   "source": [
    "accuracy=W2Vaccuracy(predictions,label)\n",
    "print('accuracy= ',accuracy,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training....\n",
      "Finish training....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 5, 2])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "print('Begin training....')\n",
    "gb=GradientBoostingClassifier(max_depth=5)\n",
    "gb.fit(trainVector[:5000], t_label[:5000])\n",
    "print('Finish training....')\n",
    "predictions_gb=gb.predict(testVector)\n",
    "predictions_gb[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=  72.2 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_gb=W2Vaccuracy(predictions_gb,label)\n",
    "print('accuracy= ',accuracy_gb,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV  # Perforing grid search\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "              'max_depth': [15, 20, 25, 30, 35],\n",
    "              'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "              'cat_smooth': [1, 10, 15, 20, 35]\n",
    "}\n",
    "gbm = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         verbose = -1,\n",
    "                         learning_rate = 0.01,\n",
    "                         num_leaves = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #创建lightgbm分类器实例\n",
    "# clf = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "#                          verbose = -1,\n",
    "#                          learning_rate = 0.01,\n",
    "#                          num_leaves = 35)\n",
    "# #拟合数据来训练\n",
    "# clf = clf.fit(trainVector, t_label)\n",
    "# #预测\n",
    "# y_pred = clf.predict(testVector)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "gsearch.fit(trainVector, t_label)\n",
    "\n",
    "print(\"Best score: %0.3f\" % gsearch.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = gsearch.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 5])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=  73.9 %\n"
     ]
    }
   ],
   "source": [
    "accuracy=W2Vaccuracy(y_pred,label)\n",
    "print('accuracy= ',accuracy,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
